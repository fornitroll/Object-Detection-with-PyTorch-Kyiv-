{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data available from http://www.manythings.org/anki/\n",
    "under eng-spa.zip link.\n",
    "Inspired by:\n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "Before you can use this script you need to install a unidecode package with:\n",
    "pip3 install unidecode\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import unidecode\n",
    "\n",
    "SpipOS_token = 0\n",
    "EOS_token = 1\n",
    "UW_token = 2\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 10\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"\n",
    "    Keeping track of language vocabulary.\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"UW\"}\n",
    "        self.n_words = 3  # Count SOS and EOS\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Add each word from sentence.\n",
    "        \"\"\"\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"\n",
    "        Add a new word to a vocabulary,\n",
    "        update all the counters and indexes.\n",
    "        \"\"\"\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "def qscleaner(w):\n",
    "    \"\"\"\n",
    "    Just remove ? character from a word.\n",
    "    \"\"\"\n",
    "    w=w.replace('?','')\n",
    "    return w\n",
    "\n",
    "def isquestion(s, max_length=MAX_SENTENCE_LENGTH):\n",
    "    \"\"\"\n",
    "    Return True if sentence is valid according\n",
    "    to our criteria.\n",
    "    Here we're interested in questions that are\n",
    "    no longer than mex_legth.\n",
    "    \"\"\"\n",
    "    return len(s.split(' ')) < max_length and len(s.split(' ')) < max_length and s.find('?') != -1\n",
    "\n",
    "def clean(s, extra_cleaner=qscleaner):\n",
    "    \"\"\"\n",
    "    Clean up the whole sentence:\n",
    "    Include only words, make\n",
    "    them lower case and\n",
    "    remove any non-english characters.\n",
    "    \"\"\"\n",
    "    include_words=[]\n",
    "    for word in s.split():\n",
    "        word=word.strip().lower()\n",
    "        word=unidecode.unidecode(word)\n",
    "        word=extra_cleaner(word)\n",
    "        if word.isdigit():\n",
    "            continue\n",
    "        include_words.append(word)\n",
    "    return ' '.join(include_words)\n",
    "\n",
    "def process_file(ilang, olang, limit, sfilter=isquestion):\n",
    "    \"\"\"\n",
    "    Read a language file, clean up sentences\n",
    "    and based on them create a Vocab object for\n",
    "    each language, return only limit sentences.\n",
    "    \"\"\"\n",
    "    print(\"Reading sentences...\")\n",
    "    sentences = open('data/%s.txt' % olang, encoding='utf-8').read().splitlines()\n",
    "    pairs = [[clean(w) for w in s.split('\\t')] for s in sentences if sfilter(s)]\n",
    "    pairs = [list(p) for p in pairs]\n",
    "    input_lang = Vocab(ilang)\n",
    "    output_lang = Vocab(olang)\n",
    "    return pairs, input_lang, output_lang\n",
    "\n",
    "def get_data(ilang, olang, limit=100, log=print):\n",
    "    \"\"\"\n",
    "    Return a limit number of sentences of both ilang and olang.\n",
    "    Sentences has to match criteria defined by sfilter and\n",
    "    are processed by wclean.\n",
    "    ilang - input language that we want to translate from\n",
    "    olang - output language that we want to tranlate to\n",
    "    limit - a number of sentences to process for each language\n",
    "            choose small number if you don't have GPU processing power\n",
    "    \"\"\"\n",
    "    pairs, input_lang, output_lang = process_file(ilang, olang, limit)\n",
    "    log(\"Got %d sentences in both langs\" % len(pairs))\n",
    "    pairs = [ pair for pair in pairs if pair][:limit]\n",
    "    log(\"Reduced to %d sentences\" % len(pairs))\n",
    "    log(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.add_sentence(pair[0])\n",
    "        output_lang.add_sentence(pair[1])\n",
    "    log(\"Counted words:\")\n",
    "    log(input_lang.name, input_lang.n_words)\n",
    "    log(output_lang.name, output_lang.n_words)\n",
    "    log('Random data sample:')\n",
    "    log(random.choice(pairs))\n",
    "    return pairs,input_lang, output_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "#from prep import get_data, MAX_SENTENCE_LENGTH, EOS_token, SOS_token, UW_token\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading sentences...\n",
      "Got 7926 sentences in both langs\n",
      "Reduced to 100 sentences\n",
      "Counting words...\n",
      "Counted words:\n",
      "en 82\n",
      "spa 118\n",
      "Random data sample:\n",
      "['am i right', 'tengo razon']\n"
     ]
    }
   ],
   "source": [
    "hidden_size=256\n",
    "pairs, input_lang, output_lang=get_data('en','spa', limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['was it fun', 'era divertido']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['why me', 'por que yo']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class DecoderGRU(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building two GRUs, encoder and decoder.\n",
    "encoder = EncoderGRU(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = DecoderGRU(hidden_size, output_lang.n_words).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_idx(lang, sentence):\n",
    "    \"\"\"\n",
    "    Encode sentences to indexes in our Vocabulary object\n",
    "    for a given langauge.\n",
    "    \"\"\"\n",
    "    out=[]\n",
    "    for word in sentence.split(' '):\n",
    "        if word in lang.word2index:\n",
    "            out.append(lang.word2index[word])\n",
    "        else:\n",
    "            out.append(UW_token)\n",
    "    return out\n",
    "\n",
    "def sentence_to_tensor(lang, sentence):\n",
    "    \"\"\"\n",
    "    Turn a sentence into a tensor.\n",
    "    Add EOS_token at the end of the new tensor\n",
    "    to mark end of the sentence.\n",
    "    \"\"\"\n",
    "    indexes = sentence_to_idx(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    #print('Sentence->Word indexes', sentence, indexes)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def pair_to_tensor(il, ol, pair):\n",
    "    \"\"\"\n",
    "    Turn a pair of sentences into a pair of tensors.\n",
    "    \"\"\"\n",
    "    input_tensor = sentence_to_tensor(il, pair[0])\n",
    "    output_tensor = sentence_to_tensor(ol, pair[1])\n",
    "    return (input_tensor, output_tensor)\n",
    "\n",
    "def train(input_tensor, output_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_func, max_length=MAX_SENTENCE_LENGTH):\n",
    "    \"\"\"\n",
    "    Encode input_tensor and feed the output to decode the output_tensor.\n",
    "    \"\"\"\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    # Forward pass, process input tensor vi encoder:\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "    # Prepare input values for decoder, starting with start of the sentence character.\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    # Make encoder output decoder's input.\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    output_length = output_tensor.size(0)\n",
    "    loss = 0\n",
    "    # Now processing output tensor via decoder:\n",
    "    for di in range(output_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        # Return the best guess for current word.\n",
    "        _, topi = decoder_output.topk(1)\n",
    "        # Prepare next input from cuurent output\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        # Calculate loss.\n",
    "        loss += loss_func(decoder_output, output_tensor[di])\n",
    "\n",
    "        # Stop if it's the end of the sentence.\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "    # Clean up the \"gradients\" before\n",
    "    # propagating changes to our network.\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Accumulate changes.\n",
    "    loss.backward()\n",
    "\n",
    "    # Propagate changes.\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / output_length\n",
    "\n",
    "def train_all(pairs, encoder, decoder, il, ol, s_epochs, print_every=10):\n",
    "    \"\"\"\n",
    "    Train on a s_epochs random pair of sentences using encoder\n",
    "    and decoder.\n",
    "    print_every - show stats on print_every sentence\n",
    "    \"\"\"\n",
    "    loss_total = 0\n",
    "\n",
    "    # Initialize optimizers for both networks.\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters())\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters())\n",
    "\n",
    "    # Get a n_iters random sentences for training.\n",
    "    training_pairs = [pair_to_tensor(il, ol, random.choice(pairs)) for i in range(s_epochs)]\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Feed each pair to both of our networks\n",
    "    for se in range(s_epochs):\n",
    "        # Get the next pair of sentences to train\n",
    "        training_pair = training_pairs[se]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        # Do the actual training.\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_func)\n",
    "        loss_total += loss\n",
    "\n",
    "        if se % print_every == 0:\n",
    "            loss_avg = loss_total / print_every\n",
    "            loss_total = 0\n",
    "            print('%d %d%% %.4f' % (se, se / s_epochs * 100, loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder, decoder, sentence, input_lang, output_lang, max_length=MAX_SENTENCE_LENGTH):\n",
    "    \"\"\"\n",
    "    Generate translation of a sentence using encoder and decoder.\n",
    "    \"\"\"\n",
    "    # This is not training, so we can\n",
    "    # save up some memory.\n",
    "    with torch.no_grad():\n",
    "        # Prepare sentence for translation.\n",
    "        input_tensor = sentence_to_tensor(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "\n",
    "        # This is similar to training, but without running\n",
    "        # .zero_grad(), .backward(), .set()\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        # First encode our sentence using the first network.\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
    "\n",
    "        # Prepare data for the second network based on the output\n",
    "        # of the first one.\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # Get the translation using the second network\n",
    "        # decode it's output.\n",
    "        decoded_words = []\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            # Get the best guess.\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            # We're done once we get at the end of the sentence.\n",
    "            if topi.item() == EOS_token:\n",
    "                break\n",
    "            else:\n",
    "                # otherwise just turn the encoded translation (as indexes)\n",
    "                # back to their respective words.\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            # Preparing the next input for decoder.\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words\n",
    "\n",
    "def test_random(encoder, decoder,ilang,olang, n=10):\n",
    "    \"\"\"\n",
    "    Randomly get a pair of sentences and compare them\n",
    "    with our translation.\n",
    "    \"\"\"\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('Question in %s: %s' % (ilang.name, pair[0].ljust(20)))\n",
    "        print('Question in %s: %s' % (olang.name, pair[1].ljust(20)))\n",
    "        output_words = test(encoder, decoder, pair[0], ilang, olang)\n",
    "        output_sentence = ' '.join(output_words).strip()\n",
    "        tick='V' if output_sentence == pair[1] else 'X'\n",
    "        print('Our guess:%s %s' % (output_sentence.ljust(20), tick))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "UW_token = 2\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0% 0.0002\n",
      "100 7% 0.4286\n",
      "200 14% 0.3810\n",
      "300 21% 0.3278\n",
      "400 28% 0.4159\n",
      "500 35% 0.3132\n",
      "600 42% 0.3503\n",
      "700 50% 0.4438\n",
      "800 57% 0.4513\n",
      "900 64% 0.3618\n",
      "1000 71% 0.5389\n",
      "1100 78% 0.3212\n",
      "1200 85% 0.3883\n",
      "1300 92% 0.2942\n"
     ]
    }
   ],
   "source": [
    "train_all(pairs,encoder,decoder,input_lang,output_lang,1400,print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving both models...\n",
      "Testing with random data...\n",
      "Question in en: is it here          \n",
      "Question in spa: aqui                \n",
      "Our guess:es                   X\n",
      "\n",
      "Question in en: are you ok          \n",
      "Question in spa: estas bien          \n",
      "Our guess:estas bien           V\n",
      "\n",
      "Question in en: who is he           \n",
      "Question in spa: quien es el         \n",
      "Our guess:quien es el          V\n",
      "\n",
      "Question in en: really              \n",
      "Question in spa: la verdad           \n",
      "Our guess:en serio             X\n",
      "\n",
      "Question in en: hi, guys.           \n",
      "Question in spa: hola, que hay       \n",
      "Our guess:que pasa, troncos    X\n",
      "\n",
      "Question in en: who cares           \n",
      "Question in spa: a quien le importa  \n",
      "Our guess:a quien le importa   V\n",
      "\n",
      "Question in en: what for            \n",
      "Question in spa: para que            \n",
      "Our guess:para que             V\n",
      "\n",
      "Question in en: how is tom          \n",
      "Question in spa: como esta tom       \n",
      "Our guess:como esta tom        V\n",
      "\n",
      "Question in en: who fell            \n",
      "Question in spa: quien se callo      \n",
      "Our guess:quien se callo       V\n",
      "\n",
      "Question in en: who swam            \n",
      "Question in spa: quien nado          \n",
      "Our guess:quien nado           V\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Saving both models...')\n",
    "torch.save(encoder.state_dict(), 'encoder.ckpt')\n",
    "torch.save(decoder.state_dict(), 'decoder.ckpt')\n",
    "print('Testing with random data...')\n",
    "test_random(encoder, decoder, input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building two GRUs, encoder and decoder.\n",
    "    encoder = EncoderGRU(input_lang.n_words, hidden_size).to(device)\n",
    "    decoder = DecoderGRU(hidden_size, output_lang.n_words).to(device)\n",
    "    print('Training models...')\n",
    "    train_all(pairs,encoder,decoder,input_lang,output_lang,900,print_every=100)\n",
    "    print('Saving both models...')\n",
    "    torch.save(encoder.state_dict(), 'encoder.ckpt')\n",
    "    torch.save(decoder.state_dict(), 'decoder.ckpt')\n",
    "    print('Testing with random data...')\n",
    "    test_random(encoder, decoder, input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_to_tensor(il, ol, pair):\n",
    "    \"\"\"\n",
    "    Turn a pair of sentences into a pair of tensors.\n",
    "    \"\"\"\n",
    "    input_tensor = sentence_to_tensor(il, pair[0])\n",
    "    output_tensor = sentence_to_tensor(ol, pair[1])\n",
    "    return (input_tensor, output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11],\n",
       "        [12],\n",
       "        [13],\n",
       "        [ 1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_to_tensor(input_lang, output_lang, pairs[5])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7],\n",
       "        [8],\n",
       "        [1]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_to_tensor(input_lang, output_lang, pairs[5])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.initHidden().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209.32271735289507"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(146**2 + 150**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
